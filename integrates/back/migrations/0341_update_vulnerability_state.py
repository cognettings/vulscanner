# pylint: disable=invalid-name
# type: ignore
"""
update state for all the vulnerabilities
First Execution Time:     2022-12-24 at 00:15:17 UTC
First Finalization Time:  2022-12-24 at 02:34:55 UTC
Second Execution Time:    2022-12-24 at 16:42:02 UTC
Second Finalization Time: 2022-12-24 at 20:00:57 UTC
"""
from aioextensions import (
    collect,
    run,
)
from boto3.dynamodb.conditions import (
    Attr,
    Key,
)
from botocore.exceptions import (
    ConnectTimeoutError,
    ReadTimeoutError,
)
from custom_exceptions import (
    VulnNotFound,
)
from dataloaders import (
    Dataloaders,
    get_new_context,
)
from db_model import (
    TABLE,
)
from db_model.utils import (
    get_as_utc_iso_format,
)
from db_model.vulnerabilities.types import (
    Vulnerability,
)
from decorators import (
    retry_on_exceptions,
)
from dynamodb import (
    keys,
    operations,
)
from dynamodb.exceptions import (
    ConditionalCheckFailedException,
)
from itertools import (
    chain,
)
import logging
import logging.config
from organizations import (
    domain as orgs_domain,
)
from settings import (
    LOGGING,
)
import time

logging.config.dictConfig(LOGGING)

# Constants
LOGGER = logging.getLogger(__name__)
LOGGER_CONSOLE = logging.getLogger("console")


async def _get_vulnerability(*, vulnerability_id: str) -> dict:
    primary_key = keys.build_key(
        facet=TABLE.facets["vulnerability_metadata"],
        values={"id": vulnerability_id},
    )

    key_structure = TABLE.primary_key
    response = await operations.query(
        condition_expression=(
            Key(key_structure.partition_key).eq(primary_key.partition_key)
            & Key(key_structure.sort_key).begins_with(primary_key.sort_key)
        ),
        facets=(TABLE.facets["vulnerability_metadata"],),
        limit=1,
        table=TABLE,
    )

    if not response.items:
        raise VulnNotFound()

    return response.items[0]


async def _get_historic_state(
    *,
    vulnerability_id: str,
) -> tuple[dict, ...]:
    primary_key = keys.build_key(
        facet=TABLE.facets["vulnerability_historic_state"],
        values={"id": vulnerability_id},
    )
    key_structure = TABLE.primary_key
    response = await operations.query(
        condition_expression=(
            Key(key_structure.partition_key).eq(primary_key.partition_key)
            & Key(key_structure.sort_key).begins_with(primary_key.sort_key)
        ),
        facets=(TABLE.facets["vulnerability_historic_state"],),
        table=TABLE,
    )

    return response.items


@retry_on_exceptions(
    exceptions=(ReadTimeoutError, ConnectTimeoutError),
    sleep_seconds=3,
)
async def process_vulnerability(
    loaders: Dataloaders, vulnerability: Vulnerability
) -> None:
    key_structure = TABLE.primary_key
    primary_key = keys.build_key(
        facet=TABLE.facets["vulnerability_metadata"],
        values={
            "finding_id": vulnerability.finding_id,
            "id": vulnerability.id,
        },
    )
    item = {
        "state.status": vulnerability.state.status.value,
    }
    try:
        vuln_ = await _get_vulnerability(vulnerability_id=vulnerability.id)
    except VulnNotFound:
        return

    if vuln_["state"]["status"] in {"OPEN", "CLOSED"}:
        LOGGER_CONSOLE.info(
            "metadata have to update",
            extra=dict(
                extra=dict(
                    partition_key=primary_key.partition_key,
                    sort_key=primary_key.sort_key,
                )
            ),
        )
        try:
            await operations.update_item(
                condition_expression=Attr(
                    key_structure.partition_key
                ).exists(),
                item=item,
                key=primary_key,
                table=TABLE,
            )
        except ConditionalCheckFailedException as ex:
            LOGGER_CONSOLE.info(
                "metadata exception",
                extra=dict(
                    extra=dict(
                        partition_key=primary_key.partition_key, exception=ex
                    )
                ),
            )
    else:
        LOGGER_CONSOLE.info(
            "metadata not to have to update",
            extra=dict(extra=dict(partition_key=primary_key.partition_key)),
        )

    historic = await loaders.vulnerability_historic_state.load(
        vulnerability.id
    )
    historic_ = await _get_historic_state(vulnerability_id=vulnerability.id)
    for state, state_ in zip(historic, historic_):
        item = {
            "status": state.status.value,
        }
        state_key = keys.build_key(
            facet=TABLE.facets["vulnerability_historic_state"],
            values={
                "id": vulnerability.id,
                "iso8601utc": get_as_utc_iso_format(state.modified_date),
            },
        )
        if state_["status"] in {"OPEN", "CLOSED"}:
            LOGGER_CONSOLE.info(
                "historic have to update",
                extra=dict(
                    extra=dict(
                        partition_key=state_key.partition_key,
                        sort_key=state_key.sort_key,
                    )
                ),
            )
            try:
                await operations.update_item(
                    condition_expression=Attr(
                        key_structure.partition_key
                    ).exists()
                    & Attr(key_structure.sort_key).eq(state_key.sort_key),
                    item=item,
                    key=state_key,
                    table=TABLE,
                )
            except ConditionalCheckFailedException as ex:
                state_key = keys.build_key(
                    facet=TABLE.facets["vulnerability_historic_state"],
                    values={
                        "id": vulnerability.id,
                        "iso8601utc": state_["sk"].split("#")[1],
                    },
                )
                LOGGER_CONSOLE.info(
                    "historic exception",
                    extra=dict(
                        extra=dict(
                            partition_key=state_key.partition_key,
                            sort_key=state_key.sort_key,
                            exception=ex,
                        )
                    ),
                )
                await operations.update_item(
                    condition_expression=Attr(
                        key_structure.partition_key
                    ).exists()
                    & Attr(key_structure.sort_key).eq(state_key.sort_key),
                    item=item,
                    key=state_key,
                    table=TABLE,
                )
        else:
            LOGGER_CONSOLE.info(
                "historic not to have to update",
                extra=dict(extra=dict(partition_key=state_key.partition_key)),
            )


@retry_on_exceptions(
    exceptions=(ReadTimeoutError, ConnectTimeoutError),
    sleep_seconds=3,
)
async def get_finding_vulnerabilities(
    loaders: Dataloaders,
    finding_id: str,
) -> tuple[Vulnerability, ...]:
    vulnerabilities = await loaders.finding_vulnerabilities_all.load(
        finding_id
    )
    return vulnerabilities


async def process_group(group_name: str) -> None:
    loaders: Dataloaders = get_new_context()
    findings = await loaders.group_drafts_and_findings.load(group_name)
    vulnerabilities = tuple(
        chain.from_iterable(
            await collect(
                tuple(
                    get_finding_vulnerabilities(loaders, finding.id)
                    for finding in findings
                ),
                workers=128,
            )
        )
    )
    await collect(
        tuple(
            process_vulnerability(loaders, vulnerability)
            for vulnerability in vulnerabilities
        ),
        workers=512,
    )
    LOGGER_CONSOLE.info(
        "Group processed",
        extra={
            "extra": {
                "group_name": group_name,
                "all_vulnerabilities": len(vulnerabilities),
            }
        },
    )


async def main() -> None:
    loaders: Dataloaders = get_new_context()
    count = 0
    groups = []
    async for organization in orgs_domain.iterate_organizations():
        org_groups = await loaders.organization_groups.load(organization.id)
        groups.extend(org_groups)
    for group in groups:
        count += 1
        LOGGER_CONSOLE.info(
            "Group",
            extra={
                "extra": {
                    "group_name": group.name,
                    "count": count,
                }
            },
        )
        await process_group(group.name)


if __name__ == "__main__":
    execution_time = time.strftime(
        "Execution Time:    %Y-%m-%d at %H:%M:%S %Z"
    )
    run(main())
    finalization_time = time.strftime(
        "Finalization Time: %Y-%m-%d at %H:%M:%S %Z"
    )
    print(f"{execution_time}\n{finalization_time}")
