from aioextensions import (
    collect,
)
from api.types import (
    APP_EXCEPTIONS,
)
from context import (
    FI_ENVIRONMENT,
    FI_TEST_PROJECTS,
)
from custom_exceptions import (
    UnableToSendMail,
)
from custom_utils import (
    cvss as cvss_utils,
)
from custom_utils.analytics import (
    get_mixpanel_events,
)
from dataloaders import (
    Dataloaders,
    get_new_context,
)
from datetime import (
    datetime,
    timedelta,
)
from db_model.enums import (
    GitCloningStatus,
)
from db_model.groups.enums import (
    GroupManaged,
)
from db_model.groups.types import (
    Group,
)
from db_model.roots.enums import (
    RootStatus,
)
from db_model.roots.types import (
    GitRoot,
    GitRootCloning,
)
from db_model.vulnerabilities.types import (
    Vulnerability,
)
from decorators import (
    retry_on_exceptions,
)
from findings import (
    domain as findings_domain,
)
import logging
import logging.config
from mailchimp_transactional.api_client import (
    ApiClientError,
)
from mailer.findings import (
    send_mail_vulnerability_report,
)
from mailer.trial import (
    send_trial_first_scanning,
)
from organizations import (
    domain as orgs_domain,
)
import pytz
from settings import (
    LOGGING,
)
from vulnerabilities.domain.core import (
    get_reported_vulnerabilities,
)

logging.config.dictConfig(LOGGING)

# Constants
LOGGER = logging.getLogger(__name__)


mail_vulnerability_report = retry_on_exceptions(
    exceptions=(UnableToSendMail, ApiClientError),
    max_attempts=3,
    sleep_seconds=2,
)(send_mail_vulnerability_report)

mail_trial_first_scanning = retry_on_exceptions(
    exceptions=(UnableToSendMail, ApiClientError),
    max_attempts=3,
    sleep_seconds=2,
)(send_trial_first_scanning)


def get_report_hour() -> int:
    current_hour = datetime.now(pytz.UTC).hour
    if 0 <= current_hour < 6:
        return 0
    if 6 <= current_hour < 12:
        return 6
    if 12 <= current_hour < 18:
        return 12

    return 18


def get_report_interval() -> tuple[datetime, datetime]:
    report_date = datetime.now(pytz.UTC).replace(
        hour=get_report_hour(), minute=0, second=0, microsecond=0
    )
    return report_date - timedelta(hours=6), report_date


async def process_vulnerability_report(
    loaders: Dataloaders,
    group_name: str,
    start_date: datetime,
    end_date: datetime,
) -> None:
    all_findings = await loaders.group_findings.load(group_name.lower())
    for finding in all_findings:
        try:
            responsible = finding.hacker_email
            vulnerabilities = await get_reported_vulnerabilities(
                loaders, finding.id, start_date, end_date, is_from_machine=True
            )
            severity_score = cvss_utils.get_vulnerabilities_score(
                finding, vulnerabilities
            )
            severity_level = cvss_utils.get_severity_level(severity_score)
            group_name = finding.group_name
            vulns_props: dict[str, dict[str, dict[str, str]]] = {}
            for vuln in vulnerabilities:
                responsible = vuln.hacker_email
                repo = await findings_domain.repo_subtitle(
                    loaders, vuln, group_name
                )
                vuln_dict = vulns_props.get(repo, {})
                vuln_dict.update(
                    {
                        f"{vuln.state.where}{vuln.state.specific}": {
                            "location": vuln.state.where,
                            "specific": vuln.state.specific,
                            "source": vuln.state.source.value,
                        },
                    }
                )
                vulns_props[repo] = dict(sorted(vuln_dict.items()))

            if vulns_props:
                await mail_vulnerability_report(
                    loaders=loaders,
                    group_name=group_name,
                    finding_title=finding.title,
                    finding_id=finding.id,
                    vulnerabilities_properties=vulns_props,
                    responsible=responsible,
                    severity_score=severity_score,
                    severity_level=severity_level,
                )

        except APP_EXCEPTIONS:
            LOGGER.info(
                "Attempted to send vulnerability report in finding",
                extra={"extra": {"finding_id": finding.id}},
            )
            continue

    LOGGER.info("Vulnerability report processed for group %s", group_name)


async def get_oldest_vuln(
    loaders: Dataloaders,
    group_name: str,
) -> Vulnerability | None:
    all_findings = await loaders.group_findings.load(group_name.lower())
    oldest_vuln = None
    if not all_findings:
        return oldest_vuln

    for finding in all_findings:
        try:
            vulnerabilities = await loaders.finding_vulnerabilities.load(
                finding.id
            )
            for vuln in vulnerabilities:
                oldest_vuln = (
                    vuln
                    if oldest_vuln is None
                    or vuln.created_date < oldest_vuln.created_date
                    else oldest_vuln
                )

        except APP_EXCEPTIONS:
            LOGGER.info(
                "Attempted to get oldest vulnerability in finding",
                extra={
                    "extra": {
                        "finding_id": finding.id,
                        "group_name": group_name,
                    }
                },
            )
            continue

    return oldest_vuln


async def get_root_historic_cloning(
    loaders: Dataloaders, root_id: str
) -> list[GitRootCloning]:
    return await loaders.root_historic_cloning.load(root_id)


async def get_roots_cloning_info(
    loaders: Dataloaders, group_name: str
) -> dict[str, list[GitRootCloning]]:
    all_roots = tuple(
        root
        for root in await loaders.group_roots.load(group_name)
        if isinstance(root, GitRoot)
        and root.state.status == RootStatus.ACTIVE
        and root.cloning.status
    )
    roots_historic_cloning = await collect(
        [get_root_historic_cloning(loaders, root.id) for root in all_roots]
    )
    roots_cloning_info = {
        git_root.id: historic_cloning
        for git_root, historic_cloning in zip(
            all_roots, roots_historic_cloning
        )
    }
    return roots_cloning_info


def get_cloning_datetimes(
    roots_cloning_info: dict[str, list[GitRootCloning]],
    status: GitCloningStatus,
) -> list[datetime]:
    return sorted(
        sum(
            [
                [
                    cloning_info.modified_date
                    for cloning_info in root_cloning_info
                    if cloning_info.status == status
                ]
                for root_cloning_info in roots_cloning_info.values()
            ],
            [],
        )
    )


async def process_first_scanning_report(
    loaders: Dataloaders,
    group: Group,
    start_date: datetime,
    end_date: datetime,
) -> None:
    oldest_vuln = await get_oldest_vuln(loaders, group.name)
    if oldest_vuln and oldest_vuln.created_date < start_date:
        return
    if oldest_vuln and oldest_vuln.created_date < end_date:
        await mail_trial_first_scanning(
            loaders=loaders,
            email_to=[group.created_by],
            have_vulns=True,
            group_name=group.name,
        )
        LOGGER.info(
            "Send mail first scanning with vulnerabilities found for %s group",
            group.name,
        )
        return
    roots_cloning_info = await get_roots_cloning_info(loaders, group.name)
    LOGGER.info(
        "Processing First vulnerability report for group %s",
        group.name,
        extra={
            "extra": {
                "roots_info": roots_cloning_info,
                "oldest_vuln": oldest_vuln,
                "machine": group.state.has_machine,
            }
        },
    )
    success_cloning_datetimes = get_cloning_datetimes(
        roots_cloning_info, GitCloningStatus.OK
    )
    if (
        success_cloning_datetimes
        and start_date
        <= success_cloning_datetimes[0] + timedelta(hours=6)
        <= end_date
    ):
        await send_trial_first_scanning(
            loaders=loaders,
            email_to=[group.created_by],
            have_vulns=False,
            group_name=group.name,
            cloning_status=GitCloningStatus.OK,
        )
        LOGGER.info(
            "Send mail first scanning for %s group",
            group.name,
        )
        return
    failed_cloning_datetimes = get_cloning_datetimes(
        roots_cloning_info, GitCloningStatus.FAILED
    )
    if (
        failed_cloning_datetimes
        and not success_cloning_datetimes
        and start_date
        <= failed_cloning_datetimes[0] + timedelta(hours=6)
        <= end_date
    ):
        await send_trial_first_scanning(
            loaders=loaders,
            email_to=[group.created_by],
            have_vulns=False,
            group_name=group.name,
            cloning_status=GitCloningStatus.FAILED,
        )
        LOGGER.info(
            "Send mail first scanning failed for %s group",
            group.name,
        )


async def process_onboard_reports() -> None:
    results = await get_mixpanel_events(
        ["AutoenrollCorporateOnly"],
    )
    LOGGER.info(
        "Processing onboard reports",
        extra={
            "extra": {
                "results": results,
            }
        },
    )


async def main() -> None:
    loaders: Dataloaders = get_new_context()
    groups = await orgs_domain.get_all_active_groups(loaders)

    if FI_ENVIRONMENT == "production":
        groups = [
            group
            for group in groups
            if group.name not in FI_TEST_PROJECTS.split(",")
        ]

    start_date, end_date = get_report_interval()
    LOGGER.info(
        "report date",
        extra={
            "extra": {
                "start_date": start_date,
                "end_date": end_date,
            }
        },
    )
    await collect(
        [
            process_vulnerability_report(
                loaders, group.name, start_date, end_date
            )
            for group in groups
            if group.state.has_squad
        ],
        workers=20,
    )
    LOGGER.info(
        "Vulnerability report for non-squad groups is being processed",
    )
    await collect(
        [
            process_vulnerability_report(
                loaders, group.name, start_date, end_date
            )
            for group in groups
            if not group.state.has_squad
        ],
        workers=20,
    )
    await collect(
        [
            process_first_scanning_report(loaders, group, start_date, end_date)
            for group in groups
            if group.state.managed == GroupManaged.TRIAL
        ],
        workers=20,
    )
    await process_onboard_reports()
