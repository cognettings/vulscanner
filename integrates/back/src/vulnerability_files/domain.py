from aioextensions import (
    collect,
    in_thread,
    schedule,
)
from collections.abc import (
    Sequence,
)
from custom_exceptions import (
    GroupNotFound,
    IncompleteFinding,
    InvalidCommitHash,
    InvalidFileSize,
    InvalidNewVulnStateStatus,
    InvalidPath,
    InvalidSchema,
    InvalidVulnsNumber,
    OrganizationNotFound,
    RequiredRemovalReason,
    RequiredSubmittedStatus,
    VulnerabilityCantNotChangeStatus,
)
from custom_utils import (
    cvss as cvss_utils,
    datetime as datetime_utils,
    files as files_utils,
    findings as findings_utils,
    logs as logs_utils,
    validations_deco as validations_utils,
    vulnerabilities as vulns_utils,
)
from dataloaders import (
    Dataloaders,
)
from datetime import (
    datetime,
)
from db_model import (
    vulnerabilities as vulns_model,
)
from db_model.enums import (
    Source,
)
from db_model.findings.enums import (
    FindingStatus,
)
from db_model.organization_finding_policies.enums import (
    PolicyStateStatus,
)
from db_model.organization_finding_policies.types import (
    OrgFindingPolicy,
)
from db_model.roots.types import (
    GitRoot,
)
from db_model.vulnerabilities.constants import (
    RELEASED_FILTER_STATUSES,
)
from db_model.vulnerabilities.enums import (
    VulnerabilityStateReason,
    VulnerabilityStateStatus,
    VulnerabilityTechnique,
    VulnerabilityToolImpact,
    VulnerabilityTreatmentStatus,
    VulnerabilityType,
)
from db_model.vulnerabilities.types import (
    Vulnerability,
    VulnerabilityAdvisory,
    VulnerabilityMetadataToUpdate,
    VulnerabilityState,
    VulnerabilityTool,
    VulnerabilityTreatment,
    VulnerabilityUnreliableIndicators,
)
from decimal import (
    Decimal,
)
from findings import (
    domain as findings_domain,
)
from graphql.type.definition import (
    GraphQLResolveInfo,
)
import html
from itertools import (
    chain,
)
import json
from mailer import (
    findings as findings_mail,
)
import os
from pykwalify.core import (
    Core,
)
from pykwalify.errors import (
    CoreError,
    SchemaError,
)
import re
from roots import (
    domain as roots_domain,
    utils as roots_utils,
)
from sessions import (
    domain as sessions_domain,
)
from starlette.datastructures import (
    UploadFile,
)
from typing import (
    Any,
)
import uuid
from vulnerabilities import (
    domain as vulns_domain,
)
from vulnerabilities.domain.core import (
    remove_vulnerability,
)
from vulnerabilities.domain.utils import (
    is_executable,
    validate_and_get_hash,
)
from vulnerabilities.domain.validations import (
    validate_ports_specific_deco,
    validate_stream_deco,
)
from vulnerability_files.reattacks import (
    process_reattack_from_uploaded_vulnerabilities,
)
from vulnerability_files.zero_risk import (
    process_zr_in_uploaded_vulnerabilities,
)
import yaml


def _get_treatment_new_vuln(
    *, modified_date: datetime, finding_policy: OrgFindingPolicy | None
) -> tuple[VulnerabilityTreatment, ...]:
    if (
        finding_policy
        and finding_policy.state.status == PolicyStateStatus.APPROVED
    ):
        return vulns_utils.get_treatment_from_org_finding_policy(
            modified_date=modified_date,
            user_email=finding_policy.state.modified_by,
        )
    return (
        VulnerabilityTreatment(
            modified_date=modified_date,
            status=VulnerabilityTreatmentStatus.UNTREATED,
        ),
    )


async def _add_vulnerability_to_dynamo(
    *,
    vuln_to_add: Vulnerability,
    finding_policy: OrgFindingPolicy | None,
    auto_approve: bool,
) -> str:
    """Add new vulnerability to DynamoDB."""
    if (
        auto_approve
        and vuln_to_add.state.status not in VulnerabilityStateStatus.VULNERABLE
    ):
        raise InvalidNewVulnStateStatus("open")
    if (
        not auto_approve
        and vuln_to_add.state.status not in VulnerabilityStateStatus.SUBMITTED
    ):
        raise InvalidNewVulnStateStatus("submitted")

    new_historic_treatment = _get_treatment_new_vuln(
        modified_date=vuln_to_add.state.modified_date,
        finding_policy=finding_policy,
    )
    new_id = str(uuid.uuid4())
    unreliable_indicators = VulnerabilityUnreliableIndicators(
        unreliable_source=vuln_to_add.state.source,
        unreliable_treatment_changes=vulns_utils.get_treatment_changes(
            new_historic_treatment
        ),
    )
    new_vulnerability = vuln_to_add._replace(
        id=new_id,
        treatment=new_historic_treatment[-1],
        unreliable_indicators=unreliable_indicators,
    )
    await vulns_model.add(vulnerability=new_vulnerability)
    if len(new_historic_treatment) > 1:
        await vulns_model.update_historic(
            current_value=new_vulnerability,
            historic=new_historic_treatment,
        )
    return new_id


def _deduplicate_vulns(
    vulns: list[Vulnerability],
) -> list[Vulnerability]:
    # In case there are repeated vulns, only the latest will be taken into
    # account
    return list(
        {
            validate_and_get_hash(vuln, from_yaml=True): vuln for vuln in vulns
        }.values()
    )


async def _get_vulns_to_add(
    loaders: Dataloaders,
    vulns_data_from_file: dict[str, list[dict[str, Any]]],
    group_name: str,
    finding_id: str,
    user_email: str,
) -> list[Vulnerability]:
    """Get typed vulnerabilities from the data processed from the yaml file."""
    roots = await loaders.group_roots.load(group_name)
    root_ids_by_nicknames = roots_utils.get_root_ids_by_nicknames(
        tuple(roots), only_git_roots=False
    )
    group = await loaders.group.load(group_name)
    if not group:
        raise GroupNotFound()
    organization = await loaders.organization.load(group.organization_id)
    if not organization:
        raise OrganizationNotFound()
    return _deduplicate_vulns(
        list(
            chain.from_iterable(
                _map_vulnerability_type(
                    index=index,
                    item=vuln,
                    finding_id=finding_id,
                    group_name=group_name,
                    organization_name=organization.name,
                    root_ids_by_nicknames=root_ids_by_nicknames,
                    user_email=user_email,
                    vuln_type=vuln_type,
                )
                for vuln_type in ["inputs", "lines", "ports"]
                for index, vuln in enumerate(
                    vulns_data_from_file.get(vuln_type, [])
                )
            )
        )
    )


def _format_where(vuln_type: str, where: str) -> str:
    if vuln_type == "lines":
        # Use Unix-like paths
        if where.find("\\") >= 0:
            path = where.replace("\\", "\\\\")
            raise InvalidPath(expr=f'"values": "{path}"')

    return where


@validations_utils.validate_sanitized_csv_input_deco(["specific"])
@validate_ports_specific_deco("vuln_type", "specific")
def _ungroup_specific(
    *, vuln_type: VulnerabilityType, specific: str
) -> list[str]:
    if vuln_type in (VulnerabilityType.LINES, VulnerabilityType.PORTS):
        return vulns_utils.ungroup_specific(specific)
    return (
        [specific]
        if re.match(r"(?P<specific>.*)\s\(.*\)(\s\[.*\])?$", specific)
        else [spec for spec in specific.split(",") if spec]
    )


# pylint: disable=unused-argument
@validate_stream_deco("where", "stream", "index", "vuln_type")
def _get_vuln_stream(
    where: str, stream: str, index: int, vuln_type: str
) -> list[str]:
    return stream.split(",")


# pylint: disable=too-many-locals
def _map_vulnerability_type(
    *,
    index: int,
    item: dict[str, Any],
    finding_id: str,
    group_name: str,
    organization_name: str,
    root_ids_by_nicknames: dict[str, str],
    user_email: str,
    vuln_type: str,
) -> list[Vulnerability]:
    """Map fields according to vulnerability type and expand data if specific
    is a range or sequence.
    """
    where_headers = {
        "inputs": {"where": "url", "specific": "field"},
        "lines": {"where": "path", "specific": "line"},
        "ports": {"where": "host", "specific": "port"},
    }
    where: str = item[where_headers[vuln_type]["where"]]
    specific: str = item[where_headers[vuln_type]["specific"]]

    where = _format_where(vuln_type, where) if vuln_type == "lines" else where

    commit_hash = str(item["commit_hash"]) if item.get("commit_hash") else None

    vuln_stream: list[str] | None = None
    if vuln_type == "inputs":
        vuln_stream = _get_vuln_stream(
            where=where,
            stream=str(item["stream"]),
            index=index,
            vuln_type=vuln_type,
        )

    tool = (
        VulnerabilityTool(
            name=item["tool"]["name"],
            impact=VulnerabilityToolImpact[
                str(item["tool"]["impact"]).upper()
            ],
        )
        if "tool" in item
        else None
    )

    advisories = item["advisories"] if item.get("advisories") else None
    if advisories:
        advisories = (
            VulnerabilityAdvisory(
                package=item["advisories"].get("package"),
                cve=item["advisories"].get("cve"),
                vulnerable_version=item["advisories"].get(
                    "vulnerable_version"
                ),
            )
            if advisories
            else None
        )
    cvss_v3 = str(item["cvss_v3"]) if item.get("cvss_v3") else None
    if cvss_v3:
        cvss_utils.validate_cvss_vector(cvss_v3)
    severity_score = (
        cvss_utils.get_severity_score_from_cvss_vector(cvss_v3)
        if cvss_v3
        else None
    )
    cwe_ids = cvss_utils.parse_cwe_ids(item.get("cwe_ids"))

    vulnerabilities: list[Vulnerability] = []
    # Machine does not report vulnerabilities in ranges
    specific_values = _ungroup_specific(
        vuln_type=VulnerabilityType[vuln_type.upper()], specific=specific
    )

    today = datetime_utils.get_utc_now()
    for _specific in specific_values:
        # but may use characters like comma `,` in an INPUT specific field
        # Vulnerability id(UUID4) will be replaced later, once the comparison
        # with existing vulns is done
        vulnerabilities = [
            *vulnerabilities,
            Vulnerability(
                created_by=user_email,
                created_date=today,
                cwe_ids=cwe_ids,
                finding_id=finding_id,
                group_name=group_name,
                hacker_email=user_email,
                id="",
                state=VulnerabilityState(
                    advisories=advisories,
                    modified_by=user_email,
                    modified_date=today,
                    source=Source[str(item["source"]).upper()],
                    status=VulnerabilityStateStatus[
                        vulns_utils.get_inverted_state_converted(
                            str(item["state"]).upper()
                        )
                    ],
                    tool=tool,
                    commit=commit_hash,
                    where=where,
                    specific=_specific,
                ),
                organization_name=organization_name,
                type=VulnerabilityType[vuln_type.upper()],
                root_id=roots_domain.get_root_id_by_nicknames(
                    nickname=item["repo_nickname"],
                    root_ids_by_nicknames=root_ids_by_nicknames,
                ),
                severity_score=severity_score,
                skims_method=item.get("skims_method"),
                skims_technique=item.get("skims_technique"),
                technique=item.get("technique") or None,
                hash=item.get("hash") or None,
                developer=item.get("developer"),
                stream=vuln_stream,
                tags=item.get("tags") or None,
            ),
        ]

    return vulnerabilities


async def _validate_vulnerabilities_commit_hash(
    *,
    loaders: Dataloaders,
    group: str,
    vulns_to_add: list[Vulnerability],
) -> None:
    all_roots = await loaders.group_roots.load(group)
    for vuln_to_add in vulns_to_add:
        current_root = next(
            (root for root in all_roots if root.id == vuln_to_add.root_id),
            None,
        )

        if (
            vuln_to_add.type == VulnerabilityType.LINES
            and isinstance(current_root, GitRoot)
            and current_root.cloning.commit
            and vuln_to_add.state.commit != current_root.cloning.commit
            and vuln_to_add.state.source != Source.MACHINE
        ):
            raise InvalidCommitHash()


def _sort_vulns_for_comparison(
    vulns: list[Vulnerability],
) -> list[Vulnerability]:
    """Sort vulns, opened first, nickname last"""
    # Open vulns have priority so they can be closed
    # No-Nickname vulns have priority so they can be set their nickname
    return sorted(
        vulns,
        key=lambda vuln: (
            # Open ones first
            vuln.state.status != VulnerabilityStateStatus.VULNERABLE,
            # Nickname last
            bool(vuln.root_id),
        ),
    )


def compare_with_vuln_in_db(
    vuln_in_db: Vulnerability, vuln_to_add: Vulnerability
) -> bool:
    if (
        vuln_in_db.state.status == VulnerabilityStateStatus.VULNERABLE
        or vuln_in_db.state.status == VulnerabilityStateStatus.SUBMITTED
        or vuln_in_db.state.status == VulnerabilityStateStatus.REJECTED
        or (
            vuln_in_db.state.status == VulnerabilityStateStatus.SAFE
            and vuln_to_add.state.status == VulnerabilityStateStatus.SAFE
        )
    ):
        return validate_and_get_hash(
            vuln_to_add, validate_root=bool(vuln_in_db.root_id)
        ) == validate_and_get_hash(
            vuln_in_db, validate_root=bool(vuln_in_db.root_id), from_yaml=True
        ) or validate_and_get_hash(
            vuln_to_add, validate_root=bool(vuln_in_db.root_id)
        ) == hash(
            vuln_in_db
        )

    return False


async def remove_from_uploaded_vulnerabilities(
    *,
    loaders: Dataloaders,
    finding_id: str,
    user_email: str,
    remove_reason: VulnerabilityStateReason | None,
    vulns_to_add: list[Vulnerability],
    vulns_in_db: list[Vulnerability | None],
) -> None:
    vulns_to_remove = tuple(
        vuln_in_db
        for vuln_to_add, vuln_in_db in zip(vulns_to_add, vulns_in_db)
        if vuln_in_db
        and vuln_to_add.state.status is VulnerabilityStateStatus.DELETED
    )
    if vulns_to_remove and not remove_reason:
        raise RequiredRemovalReason()

    await collect(
        tuple(
            remove_vulnerability(
                loaders=loaders,
                email=user_email,
                finding_id=finding_id,
                justification=remove_reason,
                vulnerability_id=vuln.id,
            )
            for vuln in vulns_to_remove
            if remove_reason
        )
    )


async def map_vulnerabilities_to_dynamo(
    *,
    loaders: Dataloaders,
    vulns_data_from_file: dict[str, list[dict[str, Any]]],
    group_name: str,
    finding_id: str,
    auto_approve: bool,
    user_info: dict[str, str],
    finding_policy: OrgFindingPolicy | None,
    context: Any | None = None,
    remove_reason: VulnerabilityStateReason | None = None,
    raise_validation: bool = True,
) -> tuple[set[str], str]:
    """Map vulnerabilities, send them to DynamoDB, verify them and return the
    processed vulnerabilities."""
    # Vulns uploaded by the user
    vulns_to_add: list[Vulnerability] = await _get_vulns_to_add(
        loaders=loaders,
        vulns_data_from_file=vulns_data_from_file,
        group_name=group_name,
        finding_id=finding_id,
        user_email=user_info["user_email"],
    )

    # Avoid DoS
    if len(vulns_to_add) > 100 and raise_validation:
        raise InvalidVulnsNumber()

    # Vulns as they appear in the DB
    sorted_vulns = _sort_vulns_for_comparison(
        await loaders.finding_vulnerabilities.load(finding_id)
    )
    vulns_in_db: list[Vulnerability | None] = [
        next(
            (
                vuln
                for vuln in sorted_vulns
                if compare_with_vuln_in_db(vuln, vuln_to_add)
            ),
            None,
        )
        for vuln_to_add in vulns_to_add
    ]

    # Validate if lines vulnerabilities have the same commit
    # as cloning.commit as its root
    await _validate_vulnerabilities_commit_hash(
        loaders=loaders,
        group=group_name,
        vulns_to_add=vulns_to_add,
    )

    await remove_from_uploaded_vulnerabilities(
        loaders=loaders,
        finding_id=finding_id,
        user_email=user_info["user_email"],
        remove_reason=remove_reason,
        vulns_to_add=vulns_to_add,
        vulns_in_db=vulns_in_db,
    )
    vulns_in_db = process_zr_in_uploaded_vulnerabilities(
        raise_validation=raise_validation, vulns_in_db=vulns_in_db
    )

    reattacked_vulns = await process_reattack_from_uploaded_vulnerabilities(
        vulns_to_add=vulns_to_add,
        vulns_in_db=vulns_in_db,
        finding_id=finding_id,
        user_info=user_info,
        loaders=loaders,
        context=context,
    )
    (
        save_and_updated_vulns,
        message,
    ) = await save_n_update_from_uploaded_vulnerabilities(
        auto_approve=auto_approve,
        finding_id=finding_id,
        loaders=loaders,
        finding_policy=finding_policy,
        raise_validation=raise_validation,
        reattacked_vulns=reattacked_vulns,
        user_info=user_info,
        vulns_to_add=vulns_to_add,
        vulns_in_db=vulns_in_db,
    )

    return set.union(reattacked_vulns, save_and_updated_vulns), message


async def save_n_update_from_uploaded_vulnerabilities(
    *,
    auto_approve: bool,
    finding_id: str,
    loaders: Dataloaders,
    finding_policy: OrgFindingPolicy | None,
    raise_validation: bool,
    reattacked_vulns: set[str],
    user_info: dict[str, str],
    vulns_to_add: list[Vulnerability],
    vulns_in_db: Sequence[Vulnerability | None],
) -> tuple[set[str], str]:
    vulns_to_add_to_dynamo: list[Vulnerability] = [
        vuln_to_add
        for vuln_to_add, vuln_in_db in zip(vulns_to_add, vulns_in_db)
        if not vuln_in_db
        and vuln_to_add.state.status != VulnerabilityStateStatus.SAFE
    ]
    _validate_state_status(
        auto_approve, vulns_to_add, list(vulns_in_db), vulns_to_add_to_dynamo
    )
    # Validate new vulnerabilities exist in the toe (surface)
    vulns_to_add_to_dynamo_list: list[Vulnerability | None] = [
        await vulns_utils.validate_vulnerability_in_toe(
            loaders,
            vuln_to_add,
            index,
            raises=raise_validation,
        )
        for index, vuln_to_add in enumerate(vulns_to_add_to_dynamo)
    ]
    # Add new vulnerabilities
    added_vuln_ids = set(
        await collect(
            _add_vulnerability_to_dynamo(
                vuln_to_add=vuln_to_add,
                finding_policy=finding_policy,
                auto_approve=auto_approve,
            )
            for vuln_to_add in vulns_to_add_to_dynamo_list
            if vuln_to_add
        )
    )

    vulns_to_update = [
        (vuln_to_add, vuln_in_db)
        for vuln_to_add, vuln_in_db in zip(vulns_to_add, vulns_in_db)
        # Exclude vulns that we'll verify as those state updates
        # are handled in the verification function
        if vuln_in_db
        and vuln_in_db.id not in reattacked_vulns
        and vuln_to_add.state.status is not VulnerabilityStateStatus.DELETED
    ]
    # Update those vulns that will not be reattacked
    updated_vuln_ids = set(
        await collect(
            vulns_domain.update_metadata_and_state(
                vulnerability=vuln_in_db,
                new_metadata=VulnerabilityMetadataToUpdate(
                    root_id=vuln_to_add.root_id,
                    cwe_ids=vuln_to_add.cwe_ids,
                    severity_score=vuln_to_add.severity_score,
                    stream=vuln_to_add.stream,
                ),
                new_state=vuln_to_add.state,
                finding_policy=finding_policy,
            )
            for vuln_to_add, vuln_in_db in vulns_to_update
            if not is_already_open(vuln_to_add, vuln_in_db)
        )
    )

    await _send_mail_reports(
        loaders=loaders,
        finding_id=finding_id,
        user_email=user_info["user_email"],
        vulns_to_add_to_dynamo_list=vulns_to_add_to_dynamo_list,
    )

    return set.union(updated_vuln_ids, added_vuln_ids), (
        "Some uploaded locations are already vulnerable"
        if len(set(vuln.id for _, vuln in vulns_to_update))
        != len(updated_vuln_ids)
        else ""
    )


async def _send_mail_reports(
    loaders: Dataloaders,
    finding_id: str,
    user_email: str,
    vulns_to_add_to_dynamo_list: list[Vulnerability | None],
) -> None:
    if (
        not vulns_to_add_to_dynamo_list
        or user_email == "machine@fluidattacks.com"
    ):
        return

    finding = await findings_domain.get_finding(loaders, finding_id)

    # Released vulnerabilities
    released_vulnerabilities = [
        vuln
        for vuln in vulns_to_add_to_dynamo_list
        if vuln and vuln.state.status in RELEASED_FILTER_STATUSES
    ]

    if (
        finding.unreliable_indicators.unreliable_status
        == FindingStatus.VULNERABLE
        and released_vulnerabilities
    ):
        severity_score = cvss_utils.get_vulnerabilities_score(
            finding, released_vulnerabilities
        )
        vulns_properties: dict[
            str, Any
        ] = await findings_domain.vulns_properties(
            loaders,
            finding_id,
            released_vulnerabilities,
        )
        schedule(
            findings_mail.send_mail_vulnerability_report(
                loaders=loaders,
                group_name=finding.group_name,
                finding_title=finding.title,
                finding_id=finding_id,
                vulnerabilities_properties=vulns_properties,
                responsible=released_vulnerabilities[0].hacker_email
                if released_vulnerabilities[0]
                else finding.hacker_email,
                severity_score=severity_score,
                severity_level=cvss_utils.get_severity_level(severity_score),
            )
        )

    # Submitted vulnerabilities
    submitted_vulnerabilities = [
        vuln
        for vuln in vulns_to_add_to_dynamo_list
        if vuln and vuln.state.status is VulnerabilityStateStatus.SUBMITTED
    ]
    if submitted_vulnerabilities:
        submitted_severity_score = cvss_utils.get_vulnerabilities_score(
            finding, submitted_vulnerabilities
        )
        vulns_properties = await findings_domain.vulns_properties(
            loaders,
            finding_id,
            submitted_vulnerabilities,
        )
        schedule(
            findings_mail.send_mail_submit_vulnerability(
                loaders=loaders,
                finding=finding,
                responsible=submitted_vulnerabilities[0].hacker_email,
                vulnerabilities_properties=vulns_properties,
                severity_score=submitted_severity_score,
                severity_level=cvss_utils.get_severity_level(
                    submitted_severity_score
                ),
            )
        )


def _validate_state_status(
    auto_approve: bool,
    vulns_to_add: list[Vulnerability],
    vulns_in_db: list[Vulnerability | None],
    vulns_to_add_to_dynamo: list[Vulnerability],
) -> None:
    if not auto_approve:
        _validate_required_submitted_status(vulns_to_add_to_dynamo)
        _validate_can_change_status(vulns_to_add, vulns_in_db)


def _validate_required_submitted_status(
    vulns_to_add_to_dynamo: list[Vulnerability],
) -> None:
    for vuln in vulns_to_add_to_dynamo:
        if vuln.state.status is VulnerabilityStateStatus.VULNERABLE:
            raise RequiredSubmittedStatus(
                where=vuln.state.where, specific=vuln.state.specific
            )


def _validate_can_change_status(
    vulns_to_add: list[Vulnerability], vulns_in_db: list[Vulnerability | None]
) -> None:
    for vuln_to_add, vuln_in_db in zip(vulns_to_add, vulns_in_db):
        if (
            vuln_in_db
            and vuln_in_db.state.status is VulnerabilityStateStatus.SUBMITTED
            and vuln_to_add.state.status
            is not VulnerabilityStateStatus.SUBMITTED
        ):
            raise VulnerabilityCantNotChangeStatus(
                where=vuln_to_add.state.where,
                specific=vuln_to_add.state.specific,
                status="submitted",
            )
        if (
            vuln_in_db
            and vuln_in_db.state.status is VulnerabilityStateStatus.REJECTED
            and vuln_to_add.state.status
            not in {
                VulnerabilityStateStatus.REJECTED,
                VulnerabilityStateStatus.SUBMITTED,
            }
        ):
            raise VulnerabilityCantNotChangeStatus(
                where=vuln_to_add.state.where,
                specific=vuln_to_add.state.specific,
                status="rejected",
            )


def is_already_open(
    vuln_to_add: Vulnerability, vuln_in_db: Vulnerability
) -> bool:
    return (
        vuln_in_db.state.status is VulnerabilityStateStatus.VULNERABLE
        and vuln_to_add.state.status is VulnerabilityStateStatus.SUBMITTED
    )


async def process_file(
    info: GraphQLResolveInfo,
    file_input: UploadFile,
    finding_id: str,
    finding_policy: OrgFindingPolicy | None,
    group_name: str,
) -> tuple[set[str], str]:
    """Process a file and return the for vulnerabilities that were
    processed."""
    raw_content = await file_input.read()
    raw_content_decoded = raw_content.decode()
    file_content = html.escape(raw_content_decoded, quote=False)
    await file_input.seek(0)
    vulnerabilities = yaml.safe_load(file_content)
    user_info = await sessions_domain.get_jwt_content(info.context)
    if await validate_file_schema(vulnerabilities, info):
        for vuln_type in ["inputs", "lines", "ports"]:
            for vuln in vulnerabilities.get(vuln_type, []):
                vuln["technique"] = VulnerabilityTechnique.MPT.value
                if vuln_type == "lines":
                    vuln["technique"] = VulnerabilityTechnique.SCR.value
                elif vuln_type == "inputs" and is_executable(
                    vuln.get("field", "")
                ):
                    vuln["technique"] = VulnerabilityTechnique.RE.value

        return await map_vulnerabilities_to_dynamo(
            context=info.context,
            vulns_data_from_file=vulnerabilities,
            group_name=group_name,
            finding_id=finding_id,
            finding_policy=finding_policy,
            loaders=info.context.loaders,
            raise_validation=True,
            auto_approve=False,
            user_info=user_info,
        )
    return set(), ""


async def upload_file(
    info: GraphQLResolveInfo,
    file_input: UploadFile,
    finding_id: str,
    finding_policy: OrgFindingPolicy | None,
    group_name: str,
) -> tuple[set[str], str]:
    """Upload the vulnerabilities and return the vulnerabilities that were
    processed."""
    loaders: Dataloaders = info.context.loaders
    finding = await findings_domain.get_finding(loaders, finding_id)
    required_fields: dict[str, bool] = dict(
        evidences=any(
            bool(evidence["url"])
            for evidence in findings_utils.get_formatted_evidence(
                finding
            ).values()
        ),
        severity=cvss_utils.get_severity_score(finding.severity) > Decimal(0),
    )
    if not all(required_fields.values()):
        raise IncompleteFinding(
            [key for key, value in required_fields.items() if not value]
        )

    mib = 1048576
    if await files_utils.get_file_size(file_input) < 1 * mib:
        processed_vulnerabilities, message = await process_file(
            info,
            file_input,
            finding_id,
            finding_policy,
            group_name,
        )
    else:
        raise InvalidFileSize()
    return processed_vulnerabilities, message


async def validate_file_schema(
    vulnerabilities: Any, info: GraphQLResolveInfo
) -> bool:
    """Validate if a file has the correct schema."""
    schema_dir = os.path.dirname(os.path.abspath(__file__))
    schema_dir = os.path.join(schema_dir, "vuln_template.yaml")
    core = Core(source_data=vulnerabilities, schema_files=[schema_dir])
    is_valid = False
    try:
        await in_thread(core.validate, raise_exception=True)
        is_valid = True
    except SchemaError as ex:
        lines_of_exceptions = core.errors
        errors_values = [
            [
                getattr(x, "pattern", getattr(x, "value", "")),
                getattr(x, "path", getattr(x, "value", "")),
            ]
            for x in lines_of_exceptions
            if not hasattr(x, "key")
        ]
        errors_keys = [x for x in lines_of_exceptions if hasattr(x, "key")]
        errors_values_formated = [json.dumps(x) for x in errors_values]
        errors_keys_formated = [f'"{x.key}, {x.path}"' for x in errors_keys]
        errors_keys_joined = ",".join(errors_keys_formated)
        errors_values_joined = ",".join(errors_values_formated)
        error_value = (
            f'"values": [{errors_values_joined}], '
            f'"keys": [{errors_keys_joined}]'
        )
        logs_utils.cloudwatch_log(
            info.context,
            "Error: An error occurred validating vulnerabilities file",
        )
        raise InvalidSchema(expr=error_value) from ex
    except CoreError as ex:
        raise InvalidSchema() from ex
    return is_valid
