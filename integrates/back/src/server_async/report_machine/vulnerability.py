from aioextensions import (
    collect,
)
from collections.abc import (
    Iterable,
)
from custom_utils.cvss import (
    get_severity_score_from_cvss_vector,
)
from custom_utils.files import (
    path_is_include,
)
from custom_utils.utils import (
    ignore_advisories,
)
from custom_utils.vulnerabilities import (
    is_machine_vuln,
)
from dataloaders import (
    Dataloaders,
)
from datetime import (
    datetime,
)
from db_model.enums import (
    Source,
)
from db_model.finding_comments.enums import (
    CommentType,
)
from db_model.findings.types import (
    Finding,
)
from db_model.roots.types import (
    GitRoot,
)
from db_model.vulnerabilities.enums import (
    VulnerabilityStateReason,
    VulnerabilityStateStatus,
    VulnerabilityType,
    VulnerabilityVerificationStatus,
)
from db_model.vulnerabilities.types import (
    Vulnerability,
    VulnerabilityAdvisory,
    VulnerabilityMetadataToUpdate,
)
from db_model.vulnerabilities.update import (
    update_historic_entry,
    update_metadata,
)
from dynamodb.types import (
    Item,
)
from findings import (
    domain as findings_domain,
)
import logging
from organizations_finding_policies import (
    domain as policies_domain,
)
from server_async.report_machine.evidences import (
    upload_evidences,
)
from server_async.report_machine.finding import (
    create_finding,
    split_target_findings,
    update_finding_metadata,
)
from server_async.report_machine.snippet import (
    update_snippets,
)
from server_async.report_machine.toe_inputs import (
    ensure_toe_inputs,
)
from server_async.utils import (
    get_input_url,
    was_input_on_sarif,
    was_lines_on_sarif,
)
from unreliable_indicators.enums import (
    EntityDependency,
)
from unreliable_indicators.operations import (
    update_unreliable_indicators_by_deps,
)
from vulnerabilities.domain.utils import (
    get_path_from_integrates_vulnerability,
)
from vulnerability_files.domain import (
    map_vulnerabilities_to_dynamo,
)

# Constants
LOGGER = logging.getLogger(__name__)


def _get_path_from_apk_vulnerability(vulnerability: Item, what: str) -> str:
    if (
        (properties := vulnerability.get("properties"))
        and properties.get("technique") == "APK"
        and (message_properties := vulnerability["message"].get("properties"))
        and (details := message_properties.get("details"))
    ):
        what = " ".join(
            (
                what,
                f"(Details: {details})",
            )
        )
    return what


def _get_path_from_missing_dependecies(vulnerability: Item, what: str) -> str:
    if (
        (properties := vulnerability.get("properties"))
        and (
            properties.get("source_method")
            == "python.pip_incomplete_dependencies_list"
        )
        and (message_properties := vulnerability["message"].get("properties"))
        and (vulnerability["ruleId"] == "120")
    ):
        what = " ".join(
            (
                what,
                (
                    "(missing dependency: "
                    f"{message_properties['dependency_name']})"
                ),
            )
        )
    return what


def get_path_from_sarif_vulnerability(
    vulnerability: Item, ignore_cve: bool = False
) -> str:
    if was_input_on_sarif(vulnerability["properties"]):
        what = get_input_url(vulnerability)
    else:
        what = vulnerability["locations"][0]["physicalLocation"][
            "artifactLocation"
        ]["uri"]

    what = _get_path_from_apk_vulnerability(vulnerability, what)
    what = _get_path_from_missing_dependecies(vulnerability, what)

    if ignore_cve:
        what = ignore_advisories(what)

    return what


def get_vulns_with_reattack(  # NOSONAR
    stream: Item,
    integrates_vulnerabilities: tuple[Vulnerability, ...],
    state: str,
) -> tuple[Vulnerability, ...]:
    result: tuple[Vulnerability, ...] = ()
    for vulnerability in integrates_vulnerabilities:
        if not (
            vulnerability.verification
            and vulnerability.verification.status
            == VulnerabilityVerificationStatus.REQUESTED
        ):
            continue
        for _vuln in stream["lines"]:
            if _vuln["state"] == state and _vuln["hash"] == vulnerability.hash:
                result = (*result, vulnerability)
                break
        for _vuln in stream["inputs"]:
            if _vuln["state"] == state and _vuln["hash"] == vulnerability.hash:
                result = (*result, vulnerability)
                break

    return result


def get_advisories(vulnerability: Item) -> Item:
    if (
        (properties := vulnerability.get("properties"))
        and (technique_value := properties.get("technique"))
        and (technique_value == "SCA")
        and (vulnerability["ruleId"] in {"011", "093"})
        and (message_properties := vulnerability["message"].get("properties"))
    ):
        return {
            "package": message_properties["dependency_name"],
            "vulnerable_version": message_properties["dependency_version"],
            "cve": message_properties["cve"],
        }
    return {}


def build_vulnerabilities_stream_from_sarif(
    vulnerabilities: list[Item],
    commit_hash: str,
    repo_nickname: str,
    state: str,
) -> Item:
    return {
        "inputs": [
            {
                "field": str(
                    vuln["locations"][0]["physicalLocation"]["region"][
                        "startLine"
                    ]
                ),
                "repo_nickname": repo_nickname,
                "state": state,
                "stream": vuln["properties"]["stream"],
                "url": get_input_url(vuln, repo_nickname),
                "skims_method": vuln["properties"]["source_method"],
                "skims_technique": vuln["properties"]["technique"],
                "developer": vuln["properties"]["method_developer"],
                "source": "MACHINE",
                "hash": vuln.get("guid", 0),
                "cwe_ids": vuln["properties"]["cwe_ids"],
                "cvss_v3": vuln["properties"]["cvss"],
                "technique": vuln["properties"]["kind"],
            }
            for vuln in vulnerabilities
            if was_input_on_sarif(vuln["properties"])
        ],
        "lines": [
            {
                "commit_hash": commit_hash,
                "line": str(
                    vuln["locations"][0]["physicalLocation"]["region"][
                        "startLine"
                    ]
                ),
                "path": get_path_from_sarif_vulnerability(vuln),
                "repo_nickname": repo_nickname,
                "state": state,
                "skims_method": vuln["properties"]["source_method"],
                "skims_technique": vuln["properties"]["technique"],
                "developer": vuln["properties"]["method_developer"],
                "source": "MACHINE",
                "hash": vuln.get("guid", 0),
                "cwe_ids": vuln["properties"]["cwe_ids"],
                "cvss_v3": vuln["properties"]["cvss"],
                "technique": vuln["properties"]["kind"],
                "advisories": get_advisories(vuln),
            }
            for vuln in vulnerabilities
            if was_lines_on_sarif(vuln["properties"])
        ],
    }


def build_vulnerabilities_stream_from_integrates(
    vulnerabilities: tuple[Vulnerability, ...],
    git_root: GitRoot,
    state: str | None = None,
    commit: str | None = None,
) -> Item:
    state = state or "closed"
    return {
        "inputs": [
            {
                "field": vuln.state.specific,  # noqa
                "repo_nickname": git_root.state.nickname,
                "state": state,
                "stream": ",".join(vuln.stream or []),
                "url": vuln.state.where,
                "skims_method": vuln.skims_method,
                "skims_technique": vuln.skims_technique,
                "technique": vuln.technique,
                "developer": vuln.developer,
                "source": vuln.state.source.value,
                "hash": vuln.hash,
                "cwe_ids": vuln.cwe_ids,
                "cvss_v3": vuln.severity_score.cvss_v3
                if vuln.severity_score
                else None,
            }
            for vuln in vulnerabilities
            if vuln.type == VulnerabilityType.INPUTS
        ],
        "lines": [
            {
                "commit_hash": commit or vuln.state.commit,
                "line": vuln.state.specific,
                "path": vuln.state.where,
                "repo_nickname": git_root.state.nickname,
                "state": state,
                "skims_method": vuln.skims_method,
                "skims_technique": vuln.skims_technique,
                "technique": vuln.technique,
                "developer": vuln.developer,
                "source": vuln.state.source.value,
                "hash": vuln.hash,
                "cwe_ids": vuln.cwe_ids,
                "cvss_v3": vuln.severity_score.cvss_v3
                if vuln.severity_score
                else None,
                "advisories": {
                    "package": vuln.state.advisories.package,
                    "vulnerable_version": (
                        vuln.state.advisories.vulnerable_version
                    ),
                    "cve": vuln.state.advisories.cve,
                }
                if vuln.state.advisories
                else None,
            }
            for vuln in vulnerabilities
            if vuln.type == VulnerabilityType.LINES
        ],
    }


def machine_vulns_to_close(
    sarif_vulns: list[Item],
    existing_machine_vulns: tuple[Vulnerability, ...],
    execution_config: Item,
) -> tuple[Vulnerability, ...]:
    sarif_hashes: set[int] = {vuln["guid"] for vuln in sarif_vulns}

    return tuple(
        vuln
        for vuln in existing_machine_vulns
        # his result was not found by Skims
        if vuln.hash
        and vuln.hash not in sarif_hashes
        and (
            # the result path is included in the current analysis
            path_is_include(
                (
                    get_path_from_integrates_vulnerability(
                        vuln.state.where,
                        vuln.type,
                        ignore_cve=vuln.type == VulnerabilityType.LINES,
                    )[1]
                ).split(" ", maxsplit=1)[0],
                [
                    *(execution_config["sast"]["include"]),
                    *(execution_config["apk"]["include"]),
                ],
                [
                    *(execution_config["sast"]["exclude"]),
                    *(execution_config["apk"]["exclude"]),
                ],
            )
            if vuln.type == VulnerabilityType.LINES
            else (
                execution_config.get("dast")
                if vuln.type == VulnerabilityType.INPUTS
                else True
            )
        )
    )


async def persist_vulnerabilities(
    *,
    loaders: Dataloaders,
    group_name: str,
    git_root: GitRoot,
    finding: Finding,
    stream: Item,
    organization_name: str,
    auto_approve: bool,
    remove_reason: VulnerabilityStateReason | None = None,
) -> set[str] | None:
    finding_policy = await policies_domain.get_finding_policy_by_name(
        loaders=loaders,
        organization_name=organization_name,
        finding_name=finding.title.lower(),
    )

    if (length := (len(stream["inputs"]) + len(stream["lines"]))) > 0:
        LOGGER.info(
            "%s vulns to modify for the finding %s", length, finding.id
        )
        await ensure_toe_inputs(loaders, group_name, git_root.id, stream)
        loaders.toe_input.clear_all()
        result = await map_vulnerabilities_to_dynamo(
            loaders=loaders,
            vulns_data_from_file=stream,
            group_name=group_name,
            finding_id=finding.id,
            finding_policy=finding_policy,
            user_info={
                "user_email": "machine@fluidattacks.com",
                "first_name": "Machine",
                "last_name": "Services",
            },
            raise_validation=False,
            auto_approve=auto_approve,
            remove_reason=remove_reason,
        )

        loaders.finding.clear(finding.id)
        loaders.finding_vulnerabilities.clear(finding.id)
        return result[0]
    return None


def get_vulns_to_confirm(
    sarif_vulns: Item,
    existing_open_machine_vulns: tuple[Vulnerability, ...],
) -> dict[str, Iterable]:
    vulns_already_reported = {
        vuln.hash
        for vuln in existing_open_machine_vulns
        if (
            vuln.verification
            and vuln.verification.status
            is VulnerabilityVerificationStatus.REQUESTED
        )
    }
    return {
        "inputs": [
            vuln
            for vuln in sarif_vulns["inputs"]
            if vuln["hash"] in vulns_already_reported
        ],
        "lines": [
            vuln
            for vuln in sarif_vulns["lines"]
            if vuln["hash"] in vulns_already_reported
        ],
    }


def get_vulns_to_open_or_submit(
    sarif_vulns: Item,
    existing_machine_vulns: tuple[Vulnerability, ...],
) -> dict[str, Iterable]:
    vulns_already_reported = {vuln.hash for vuln in existing_machine_vulns}
    return {
        "inputs": [
            vuln
            for vuln in sarif_vulns["inputs"]
            if vuln["hash"] not in vulns_already_reported
        ],
        "lines": [
            vuln
            for vuln in sarif_vulns["lines"]
            if vuln["hash"] not in vulns_already_reported
        ],
    }


async def update_vulns_already_reported(  # NOSONAR
    sarif_vulns: Item,
    existing_machine_vulns: tuple[Vulnerability, ...],
) -> None:
    """Some field of the vulnerability can change, but it is still the same
    vulnerability, the status of the vulnerability must be updated.
    """
    vulns_already_reported = {
        vuln.hash: vuln
        for vuln in existing_machine_vulns
        if (
            vuln.verification is None
            or (
                vuln.verification.status
                != VulnerabilityVerificationStatus.REQUESTED
            )
        )
        and vuln.hash
    }
    futures = []
    for vuln in sarif_vulns["lines"]:
        if _current_vuln := vulns_already_reported.get(vuln["hash"]):
            if (
                (cve_in := vuln.get("advisories").get("cve"))
                and _current_vuln.state.advisories
                and (
                    cve := _current_vuln.state.advisories.cve
                    if _current_vuln.state.advisories.cve
                    else None
                )
                and (cve_in != cve)
            ):
                new_state = _current_vuln.state._replace(
                    advisories=VulnerabilityAdvisory(
                        package=_current_vuln.state.advisories.package,
                        cve=cve_in,
                        vulnerable_version=(
                            _current_vuln.state.advisories.vulnerable_version
                        ),
                    ),
                    modified_date=datetime.utcnow(),
                    source=Source.MACHINE,
                    reasons=[VulnerabilityStateReason.CONSISTENCY],
                    modified_by="machine@fluidattacks.com",
                    commit=vuln["commit_hash"],
                )
                futures.append(
                    update_historic_entry(
                        current_value=_current_vuln,
                        finding_id=_current_vuln.finding_id,
                        entry=new_state,
                        vulnerability_id=_current_vuln.id,
                    )
                )
            if (
                (upd_cvss := vuln.get("cvss_v3"))
                and (
                    new_severity_score := get_severity_score_from_cvss_vector(
                        upd_cvss
                    )
                )
                and _current_vuln.severity_score != new_severity_score
            ):
                futures.append(
                    update_metadata(
                        finding_id=_current_vuln.finding_id,
                        metadata=VulnerabilityMetadataToUpdate(
                            severity_score=new_severity_score,
                        ),
                        vulnerability_id=_current_vuln.id,
                    )
                )
    await collect(futures, workers=50)


async def close_vulns_in_non_target(
    *,
    loaders: Dataloaders,
    group_name: str,
    git_root: GitRoot,
    sarif_log: Item,
    organization_name: str,
    finding: Finding,
    auto_approve: bool,
) -> None:
    existing_open_machine_vulns = tuple(
        vuln
        for vuln in await loaders.finding_vulnerabilities.load(finding.id)
        if vuln.state.status == VulnerabilityStateStatus.VULNERABLE
        and is_machine_vuln(vuln)
        and vuln.root_id == git_root.id
    )
    existing_vulns_to_close = build_vulnerabilities_stream_from_integrates(
        existing_open_machine_vulns,
        git_root,
        state="closed",
        commit=sarif_log["runs"][0]["versionControlProvenance"][0][
            "revisionId"
        ],
    )
    if persisted_vulns := await persist_vulnerabilities(
        loaders=loaders,
        group_name=group_name,
        git_root=git_root,
        finding=finding,
        stream=existing_vulns_to_close,
        organization_name=organization_name,
        auto_approve=auto_approve,
    ):
        await update_unreliable_indicators_by_deps(
            EntityDependency.upload_file,
            finding_ids=[finding.id],
            vulnerability_ids=list(persisted_vulns),
        )


async def remove_vulns_in_non_target(
    *,
    loaders: Dataloaders,
    group_name: str,
    git_root: GitRoot,
    sarif_log: Item,
    organization_name: str,
    finding: Finding,
    auto_approve: bool,
) -> None:
    existing_unreleased_machine_vulns = tuple(
        vuln
        for vuln in await loaders.finding_vulnerabilities.load(finding.id)
        if vuln.state.status
        in {
            VulnerabilityStateStatus.SUBMITTED,
            VulnerabilityStateStatus.REJECTED,
        }
        and is_machine_vuln(vuln)
        and vuln.root_id == git_root.id
    )
    existing_vulns_to_remove = build_vulnerabilities_stream_from_integrates(
        existing_unreleased_machine_vulns,
        git_root,
        state="deleted",
        commit=sarif_log["runs"][0]["versionControlProvenance"][0][
            "revisionId"
        ],
    )
    if await persist_vulnerabilities(
        loaders=loaders,
        group_name=group_name,
        git_root=git_root,
        finding=finding,
        stream=existing_vulns_to_remove,
        organization_name=organization_name,
        auto_approve=auto_approve,
        remove_reason=VulnerabilityStateReason.NOT_REQUIRED,
    ):
        await update_unreliable_indicators_by_deps(
            EntityDependency.upload_file,
            finding_ids=[finding.id],
            vulnerability_ids=[],
        )


async def process_criteria_vuln(  # pylint: disable=too-many-locals
    *,
    loaders: Dataloaders,
    group_name: str,
    vulnerability_id: str,
    criteria_vulnerability: Item,
    criteria_requirements: Item,
    language: str,
    git_root: GitRoot,
    sarif_log: Item,
    execution_config: Item,
    organization_name: str,
    same_type_of_findings: tuple[Finding, ...],
    auto_approve: bool = False,
) -> int:
    sarif_vulns = [
        vuln
        for vuln in sarif_log["runs"][0]["results"]
        if vuln["ruleId"] == vulnerability_id
    ]
    target_finding, non_target_findings = split_target_findings(
        same_type_of_findings,
    )
    if not target_finding:
        if not sarif_vulns:
            return 0
        target_finding = await create_finding(
            loaders,
            group_name,
            vulnerability_id,
            language,
            criteria_vulnerability,
        )
    else:
        await update_finding_metadata(
            (group_name, vulnerability_id, language),
            target_finding,
            criteria_vulnerability,
            criteria_requirements,
        )
        loaders.group_findings.clear(group_name)
        loaders.finding.clear(target_finding.id)

    existing_machine_vulns: tuple[Vulnerability, ...] = tuple(
        vuln
        for vuln in await loaders.finding_vulnerabilities.load(
            target_finding.id
        )
        if is_machine_vuln(vuln) and vuln.root_id == git_root.id
    )
    existing_open_machine_vulns: tuple[Vulnerability, ...] = tuple(
        vuln
        for vuln in existing_machine_vulns
        if vuln.state.status == VulnerabilityStateStatus.VULNERABLE
    )
    existing_unreleased_machine_vulns: tuple[Vulnerability, ...] = tuple(
        vuln
        for vuln in existing_machine_vulns
        if vuln.state.status
        in {
            VulnerabilityStateStatus.SUBMITTED,
            VulnerabilityStateStatus.REJECTED,
        }
    )

    existing_vulns_to_close = build_vulnerabilities_stream_from_integrates(
        machine_vulns_to_close(
            sarif_vulns,
            existing_open_machine_vulns,
            execution_config,
        ),
        git_root,
        state="closed",
        commit=sarif_log["runs"][0]["versionControlProvenance"][0][
            "revisionId"
        ],
    )
    existing_vulns_to_remove = build_vulnerabilities_stream_from_integrates(
        machine_vulns_to_close(
            sarif_vulns,
            existing_unreleased_machine_vulns,
            execution_config,
        ),
        git_root,
        state="deleted",
        commit=sarif_log["runs"][0]["versionControlProvenance"][0][
            "revisionId"
        ],
    )
    vulns_to_confirm = get_vulns_to_confirm(
        build_vulnerabilities_stream_from_sarif(
            sarif_vulns,
            sarif_log["runs"][0]["versionControlProvenance"][0]["revisionId"],
            git_root.state.nickname,
            "open",
        ),
        existing_open_machine_vulns,
    )

    reattack_future = findings_domain.add_reattack_justification(
        loaders,
        target_finding.id,
        get_vulns_with_reattack(
            vulns_to_confirm, existing_open_machine_vulns, "open"
        ),
        get_vulns_with_reattack(
            existing_vulns_to_close, existing_open_machine_vulns, "closed"
        ),
        sarif_log["runs"][0]["versionControlProvenance"][0]["revisionId"],
        CommentType.VERIFICATION,
    )

    vulns_from_sarif = build_vulnerabilities_stream_from_sarif(
        sarif_vulns,
        sarif_log["runs"][0]["versionControlProvenance"][0]["revisionId"],
        git_root.state.nickname,
        "open" if auto_approve else "submitted",
    )

    should_update_evidence: bool = False
    await update_vulns_already_reported(
        vulns_from_sarif,
        existing_open_machine_vulns + existing_unreleased_machine_vulns,
    )

    vulns_to_open_or_submit = get_vulns_to_open_or_submit(
        vulns_from_sarif,
        existing_open_machine_vulns + existing_unreleased_machine_vulns,
    )
    if persisted_vulns := await persist_vulnerabilities(
        loaders=loaders,
        group_name=group_name,
        git_root=git_root,
        finding=target_finding,
        stream={
            "inputs": [
                *vulns_to_confirm["inputs"],
                *vulns_to_open_or_submit["inputs"],
                *existing_vulns_to_close["inputs"],
                *existing_vulns_to_remove["inputs"],
            ],
            "lines": [
                *vulns_to_confirm["lines"],
                *vulns_to_open_or_submit["lines"],
                *existing_vulns_to_close["lines"],
                *existing_vulns_to_remove["lines"],
            ],
        },
        organization_name=organization_name,
        auto_approve=auto_approve,
    ):
        should_update_evidence = True
        await reattack_future
        # Update all finding indicators with latest information
        await update_unreliable_indicators_by_deps(
            EntityDependency.upload_file,
            finding_ids=[target_finding.id],
            vulnerability_ids=list(persisted_vulns),
        )
        await update_snippets(loaders, persisted_vulns, sarif_vulns)
    else:
        reattack_future.close()

    if should_update_evidence or (
        target_finding.evidences.evidence5 is None and len(sarif_vulns) > 0
    ):
        await upload_evidences(loaders, target_finding, sarif_vulns)

    await collect(
        tuple(
            close_vulns_in_non_target(
                loaders=loaders,
                group_name=group_name,
                git_root=git_root,
                sarif_log=sarif_log,
                organization_name=organization_name,
                finding=finding,
                auto_approve=auto_approve,
            )
            for finding in non_target_findings
        )
    )
    await collect(
        tuple(
            remove_vulns_in_non_target(
                loaders=loaders,
                group_name=group_name,
                git_root=git_root,
                sarif_log=sarif_log,
                organization_name=organization_name,
                finding=finding,
                auto_approve=auto_approve,
            )
            for finding in non_target_findings
        )
    )
    return sum(
        [
            len(
                [
                    *vulns_to_confirm["inputs"],
                    *vulns_to_open_or_submit["inputs"],
                    *existing_vulns_to_close["inputs"],
                    *existing_vulns_to_remove["inputs"],
                ]
            ),
            len(
                [
                    *vulns_to_confirm["lines"],
                    *vulns_to_open_or_submit["lines"],
                    *existing_vulns_to_close["lines"],
                    *existing_vulns_to_remove["lines"],
                ]
            ),
        ]
    )
